{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis: Anomaly Detection\n",
        "\n",
        "This notebook visualizes the data to spot anomalies before econometric modeling.\n",
        "\n",
        "## Objectives:\n",
        "1. Load the final panel dataset\n",
        "2. Trend Analysis: Visualize average FHTC Coverage over time\n",
        "3. Correlation Check: Examine relationship between FHTC Coverage and health outcomes\n",
        "4. Bias Inspection: Identify and visualize suspicious spikes in the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "import random\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seaborn styling\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Add parent directory to path for config imports\n",
        "sys.path.insert(0, str(Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()))\n",
        "\n",
        "try:\n",
        "    from config import FILE_PATHS\n",
        "    print(\"✓ Config imported successfully\")\n",
        "except ImportError:\n",
        "    # Fallback paths if config not available\n",
        "    FILE_PATHS = {\n",
        "        \"data\": {\n",
        "            \"raw\": \"data/raw\",\n",
        "            \"processed\": \"data/processed\"\n",
        "        }\n",
        "    }\n",
        "    print(\"⚠ Using fallback paths\")\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Data\n",
        "\n",
        "Load the final panel dataset from the processed data directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the final panel dataset\n",
        "panel_file = Path(FILE_PATHS[\"data\"][\"processed\"]) / \"final_panel.csv\"\n",
        "print(f\"Loading panel data from: {panel_file}\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(panel_file)\n",
        "    print(f\"✓ Panel data loaded successfully!\")\n",
        "    print(f\"  Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "    print(f\"\\nColumn names:\")\n",
        "    for i, col in enumerate(df.columns, 1):\n",
        "        print(f\"  {i:2d}. {col}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"⚠ Error: File not found at {panel_file}\")\n",
        "    print(\"Please run the 01_Data_Preparation notebook first to create the panel dataset.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst few rows:\")\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify key columns for analysis\n",
        "print(\"=\"*60)\n",
        "print(\"IDENTIFYING KEY COLUMNS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Find FHTC coverage column\n",
        "fhtc_col = None\n",
        "fhtc_keywords = ['fhtc', 'coverage', 'fhtc_coverage']\n",
        "for col in df.columns:\n",
        "    if any(keyword in col.lower() for keyword in fhtc_keywords):\n",
        "        fhtc_col = col\n",
        "        break\n",
        "\n",
        "if fhtc_col:\n",
        "    print(f\"✓ FHTC Coverage column found: '{fhtc_col}'\")\n",
        "else:\n",
        "    print(\"⚠ FHTC Coverage column not found. Please check column names.\")\n",
        "    print(\"Available columns:\", list(df.columns))\n",
        "\n",
        "# Find date column\n",
        "date_col = None\n",
        "date_keywords = ['date', 'time', 'period', 'month']\n",
        "for col in df.columns:\n",
        "    if any(keyword in col.lower() for keyword in date_keywords):\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[col]) or 'date' in col.lower():\n",
        "            date_col = col\n",
        "            break\n",
        "\n",
        "if date_col:\n",
        "    print(f\"✓ Date column found: '{date_col}'\")\n",
        "    # Convert to datetime if not already\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
        "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "else:\n",
        "    print(\"⚠ Date column not found. Please check column names.\")\n",
        "\n",
        "# Find district column\n",
        "district_col = None\n",
        "district_keywords = ['district', 'dist']\n",
        "for col in df.columns:\n",
        "    if any(keyword in col.lower() for keyword in district_keywords):\n",
        "        district_col = col\n",
        "        break\n",
        "\n",
        "if district_col:\n",
        "    print(f\"✓ District column found: '{district_col}'\")\n",
        "else:\n",
        "    print(\"⚠ District column not found.\")\n",
        "\n",
        "# Find suspicious_spike column\n",
        "spike_col = None\n",
        "if 'suspicious_spike' in df.columns:\n",
        "    spike_col = 'suspicious_spike'\n",
        "    print(f\"✓ Suspicious spike column found: '{spike_col}'\")\n",
        "else:\n",
        "    print(\"⚠ Suspicious spike column not found. Bias inspection may not work.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Trend Analysis\n",
        "\n",
        "Plot the average FHTC Coverage over time across all districts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trend Analysis: Average FHTC Coverage over time\n",
        "if fhtc_col and date_col:\n",
        "    print(\"=\"*60)\n",
        "    print(\"TREND ANALYSIS: Average FHTC Coverage Over Time\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Group by date and calculate average coverage\n",
        "    trend_data = df.groupby(date_col)[fhtc_col].mean().reset_index()\n",
        "    trend_data = trend_data.sort_values(date_col)\n",
        "    \n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    \n",
        "    # Plot the trend line\n",
        "    sns.lineplot(data=trend_data, x=date_col, y=fhtc_col, \n",
        "                  marker='o', linewidth=2.5, markersize=8, color='#2E86AB')\n",
        "    \n",
        "    # Add shaded confidence interval (using std)\n",
        "    if len(trend_data) > 1:\n",
        "        std_data = df.groupby(date_col)[fhtc_col].std().reset_index()\n",
        "        std_data = std_data.sort_values(date_col)\n",
        "        plt.fill_between(trend_data[date_col], \n",
        "                         trend_data[fhtc_col] - std_data[fhtc_col],\n",
        "                         trend_data[fhtc_col] + std_data[fhtc_col],\n",
        "                         alpha=0.2, color='#2E86AB', label='±1 Std Dev')\n",
        "    \n",
        "    plt.title('Average FHTC Coverage Over Time (All Districts)', \n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Date', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Average FHTC Coverage (%)', fontsize=12, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3, linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\nSummary Statistics:\")\n",
        "    print(f\"  Mean Coverage: {trend_data[fhtc_col].mean():.2f}%\")\n",
        "    print(f\"  Min Coverage: {trend_data[fhtc_col].min():.2f}%\")\n",
        "    print(f\"  Max Coverage: {trend_data[fhtc_col].max():.2f}%\")\n",
        "    print(f\"  Overall Trend: {((trend_data[fhtc_col].iloc[-1] - trend_data[fhtc_col].iloc[0]) / trend_data[fhtc_col].iloc[0] * 100):.2f}% change\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠ Cannot perform trend analysis: Missing required columns (FHTC or Date)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Correlation Check\n",
        "\n",
        "Create a scatter plot to examine the relationship between FHTC Coverage and health outcomes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify health outcome columns\n",
        "print(\"=\"*60)\n",
        "print(\"IDENTIFYING HEALTH OUTCOME COLUMNS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Common health outcome column names\n",
        "health_keywords = ['disease', 'mortality', 'morbidity', 'health', 'cases', 'rate', 'outcome']\n",
        "health_cols = [col for col in df.columns \n",
        "               if any(keyword in col.lower() for keyword in health_keywords) \n",
        "               and col != fhtc_col \n",
        "               and pd.api.types.is_numeric_dtype(df[col])]\n",
        "\n",
        "if health_cols:\n",
        "    print(f\"Found {len(health_cols)} potential health outcome columns:\")\n",
        "    for i, col in enumerate(health_cols, 1):\n",
        "        print(f\"  {i}. {col}\")\n",
        "    health_outcome_col = health_cols[0]  # Use first found\n",
        "    print(f\"\\nUsing '{health_outcome_col}' for correlation analysis\")\n",
        "else:\n",
        "    print(\"⚠ No health outcome columns automatically detected.\")\n",
        "    print(\"\\nPlease specify the health outcome column name.\")\n",
        "    print(\"Available numerical columns:\")\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    for i, col in enumerate(numeric_cols, 1):\n",
        "        if col != fhtc_col:\n",
        "            print(f\"  {i}. {col}\")\n",
        "    health_outcome_col = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation Check: Scatter plot\n",
        "if fhtc_col and health_outcome_col:\n",
        "    print(\"=\"*60)\n",
        "    print(\"CORRELATION CHECK: FHTC Coverage vs Health Outcomes\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Remove rows with missing values\n",
        "    plot_data = df[[fhtc_col, health_outcome_col]].dropna()\n",
        "    \n",
        "    # Create scatter plot with seaborn\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Scatter plot with regression line\n",
        "    sns.scatterplot(data=plot_data, x=fhtc_col, y=health_outcome_col, \n",
        "                   alpha=0.6, s=100, color='#A23B72')\n",
        "    sns.regplot(data=plot_data, x=fhtc_col, y=health_outcome_col, \n",
        "               scatter=False, color='#F18F01', line_kws={'linewidth': 2.5})\n",
        "    \n",
        "    plt.title(f'FHTC Coverage vs {health_outcome_col}', \n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('FHTC Coverage (%)', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel(health_outcome_col.replace('_', ' ').title(), fontsize=12, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate correlation\n",
        "    correlation = plot_data[fhtc_col].corr(plot_data[health_outcome_col])\n",
        "    print(f\"\\nCorrelation Coefficient: {correlation:.4f}\")\n",
        "    \n",
        "    if abs(correlation) < 0.1:\n",
        "        print(\"  Interpretation: Very weak correlation\")\n",
        "    elif abs(correlation) < 0.3:\n",
        "        print(\"  Interpretation: Weak correlation\")\n",
        "    elif abs(correlation) < 0.5:\n",
        "        print(\"  Interpretation: Moderate correlation\")\n",
        "    elif abs(correlation) < 0.7:\n",
        "        print(\"  Interpretation: Strong correlation\")\n",
        "    else:\n",
        "        print(\"  Interpretation: Very strong correlation\")\n",
        "        \n",
        "elif fhtc_col and not health_outcome_col:\n",
        "    print(\"\\n⚠ Please define the health outcome column name.\")\n",
        "    print(\"You can set it manually in the next cell or modify the code above.\")\n",
        "    \n",
        "    # Create a placeholder for manual specification\n",
        "    print(\"\\nTo manually specify, run:\")\n",
        "    print(\"  health_outcome_col = 'YOUR_COLUMN_NAME'\")\n",
        "    print(\"Then re-run the correlation check cell.\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠ Cannot perform correlation analysis: Missing required columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Bias Inspection\n",
        "\n",
        "Filter data for suspicious spikes and visualize time series for districts with anomalies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bias Inspection: Identify districts with suspicious spikes\n",
        "if spike_col and fhtc_col and date_col and district_col:\n",
        "    print(\"=\"*60)\n",
        "    print(\"BIAS INSPECTION: Suspicious Spikes Analysis\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Filter rows with suspicious spikes\n",
        "    suspicious_data = df[df[spike_col] == True].copy()\n",
        "    \n",
        "    if len(suspicious_data) > 0:\n",
        "        print(f\"\\nFound {len(suspicious_data)} rows with suspicious spikes\")\n",
        "        print(f\"Affected districts: {suspicious_data[district_col].nunique()}\")\n",
        "        \n",
        "        # Get unique districts with spikes\n",
        "        districts_with_spikes = suspicious_data[district_col].unique().tolist()\n",
        "        print(f\"\\nDistricts with suspicious spikes: {len(districts_with_spikes)}\")\n",
        "        \n",
        "        # Select 3 random districts (or all if less than 3)\n",
        "        if len(districts_with_spikes) >= 3:\n",
        "            selected_districts = random.sample(districts_with_spikes, 3)\n",
        "        else:\n",
        "            selected_districts = districts_with_spikes\n",
        "        \n",
        "        print(f\"\\nVisualizing {len(selected_districts)} district(s): {selected_districts}\")\n",
        "        \n",
        "        # Create time series plots for selected districts\n",
        "        fig, axes = plt.subplots(len(selected_districts), 1, figsize=(14, 5*len(selected_districts)))\n",
        "        if len(selected_districts) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for idx, district in enumerate(selected_districts):\n",
        "            # Get all data for this district\n",
        "            district_data = df[df[district_col] == district].copy()\n",
        "            district_data = district_data.sort_values(date_col)\n",
        "            \n",
        "            # Plot the time series\n",
        "            ax = axes[idx]\n",
        "            sns.lineplot(data=district_data, x=date_col, y=fhtc_col, \n",
        "                        marker='o', linewidth=2, markersize=6, \n",
        "                        color='#2E86AB', ax=ax, label='FHTC Coverage')\n",
        "            \n",
        "            # Highlight suspicious spikes\n",
        "            spike_points = district_data[district_data[spike_col] == True]\n",
        "            if len(spike_points) > 0:\n",
        "                ax.scatter(spike_points[date_col], spike_points[fhtc_col],\n",
        "                          color='red', s=200, marker='X', zorder=5,\n",
        "                          label='Suspicious Spike', linewidths=2)\n",
        "            \n",
        "            ax.set_title(f'FHTC Coverage Over Time: {district}', \n",
        "                        fontsize=14, fontweight='bold', pad=15)\n",
        "            ax.set_xlabel('Date', fontsize=11, fontweight='bold')\n",
        "            ax.set_ylabel('FHTC Coverage (%)', fontsize=11, fontweight='bold')\n",
        "            ax.grid(True, alpha=0.3, linestyle='--')\n",
        "            ax.legend(loc='best')\n",
        "            \n",
        "            # Add annotation for spike values\n",
        "            if len(spike_points) > 0:\n",
        "                for _, row in spike_points.iterrows():\n",
        "                    ax.annotate(f'Spike: {row[fhtc_col]:.1f}%',\n",
        "                              xy=(row[date_col], row[fhtc_col]),\n",
        "                              xytext=(10, 10), textcoords='offset points',\n",
        "                              bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
        "                              arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Summary statistics for suspicious spikes\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"SUSPICIOUS SPIKE SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\nTotal suspicious spikes: {len(suspicious_data)}\")\n",
        "        print(f\"\\nCoverage statistics for spike points:\")\n",
        "        print(suspicious_data[fhtc_col].describe())\n",
        "        \n",
        "    else:\n",
        "        print(\"\\n✓ No suspicious spikes found in the dataset!\")\n",
        "        \n",
        "else:\n",
        "    print(\"⚠ Cannot perform bias inspection: Missing required columns\")\n",
        "    if not spike_col:\n",
        "        print(\"  - Missing: suspicious_spike column\")\n",
        "    if not fhtc_col:\n",
        "        print(\"  - Missing: FHTC coverage column\")\n",
        "    if not date_col:\n",
        "        print(\"  - Missing: Date column\")\n",
        "    if not district_col:\n",
        "        print(\"  - Missing: District column\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has visualized key patterns and anomalies in the data:\n",
        "\n",
        "1. **Trend Analysis**: Shows the overall trajectory of FHTC coverage over time\n",
        "2. **Correlation Check**: Examines the relationship between coverage and health outcomes\n",
        "3. **Bias Inspection**: Identifies and visualizes suspicious data spikes\n",
        "\n",
        "These visualizations help identify data quality issues before proceeding with econometric modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\"*60)\n",
        "print(\"EXPLORATORY ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nDataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "\n",
        "if fhtc_col:\n",
        "    print(f\"\\nFHTC Coverage Statistics:\")\n",
        "    print(df[fhtc_col].describe())\n",
        "\n",
        "if spike_col and spike_col in df.columns:\n",
        "    spike_count = df[df[spike_col] == True].shape[0]\n",
        "    print(f\"\\nSuspicious Spikes: {spike_count} ({spike_count/len(df)*100:.2f}% of data)\")\n",
        "\n",
        "print(\"\\n✓ Anomaly detection visualizations complete!\")\n",
        "print(\"Review the plots above to identify data quality issues before modeling.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=\"*60)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_data.index,\n",
        "    'Missing Count': missing_data.values,\n",
        "    'Missing Percentage': missing_percent.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(f\"\\nColumns with missing values: {len(missing_df)}\")\n",
        "    display(missing_df)\n",
        "else:\n",
        "    print(\"\\n✓ No missing values found in the dataset!\")\n",
        "\n",
        "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Percentage of missing data: {(df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics for numerical columns\n",
        "print(\"=\"*60)\n",
        "print(\"DESCRIPTIVE STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Identify numerical columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"\\nNumerical columns ({len(numeric_cols)}): {numeric_cols}\")\n",
        "\n",
        "if len(numeric_cols) > 0:\n",
        "    print(\"\\nSummary Statistics:\")\n",
        "    display(df[numeric_cols].describe())\n",
        "else:\n",
        "    print(\"\\n⚠ No numerical columns found in the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for categorical/string columns\n",
        "print(\"=\"*60)\n",
        "print(\"CATEGORICAL VARIABLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(f\"\\nCategorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    for col in categorical_cols:\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Unique values: {unique_count}\")\n",
        "        if unique_count <= 20:\n",
        "            print(f\"  Values: {df[col].unique().tolist()}\")\n",
        "        else:\n",
        "            print(f\"  First 10 values: {df[col].unique()[:10].tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check date columns and time range\n",
        "print(\"=\"*60)\n",
        "print(\"TEMPORAL ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Try to identify date columns\n",
        "date_cols = []\n",
        "for col in df.columns:\n",
        "    if 'date' in col.lower() or 'time' in col.lower() or 'period' in col.lower():\n",
        "        date_cols.append(col)\n",
        "        try:\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "if len(date_cols) > 0:\n",
        "    print(f\"\\nDate columns found: {date_cols}\")\n",
        "    for col in date_cols:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
        "            print(f\"\\n{col}:\")\n",
        "            print(f\"  Date range: {df[col].min()} to {df[col].max()}\")\n",
        "            print(f\"  Unique dates: {df[col].nunique()}\")\n",
        "            print(f\"  Missing dates: {df[col].isna().sum()}\")\n",
        "else:\n",
        "    print(\"\\n⚠ No date columns identified\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for district/geographic columns\n",
        "print(\"=\"*60)\n",
        "print(\"GEOGRAPHIC ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Try to identify district/geographic columns\n",
        "geo_keywords = ['district', 'state', 'region', 'zone', 'area']\n",
        "geo_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in geo_keywords)]\n",
        "\n",
        "if len(geo_cols) > 0:\n",
        "    print(f\"\\nGeographic columns found: {geo_cols}\")\n",
        "    for col in geo_cols:\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Unique values: {unique_count}\")\n",
        "        if unique_count <= 30:\n",
        "            print(f\"  Values: {sorted(df[col].dropna().unique().tolist())}\")\n",
        "        else:\n",
        "            print(f\"  First 20 values: {sorted(df[col].dropna().unique().tolist())[:20]}\")\n",
        "else:\n",
        "    print(\"\\n⚠ No geographic columns identified\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Quality Checks\n",
        "\n",
        "Perform initial data quality assessments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "print(\"=\"*60)\n",
        "print(\"DUPLICATE RECORDS CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"\\nTotal duplicate rows: {duplicate_count}\")\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    print(f\"Percentage of duplicates: {(duplicate_count / len(df)) * 100:.2f}%\")\n",
        "    print(\"\\nSample duplicate rows:\")\n",
        "    display(df[df.duplicated()].head())\n",
        "else:\n",
        "    print(\"✓ No duplicate rows found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for outliers in numerical columns (using IQR method)\n",
        "print(\"=\"*60)\n",
        "print(\"OUTLIER DETECTION (IQR Method)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if len(numeric_cols) > 0:\n",
        "    outlier_summary = []\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "        outlier_count = len(outliers)\n",
        "        \n",
        "        if outlier_count > 0:\n",
        "            outlier_summary.append({\n",
        "                'Column': col,\n",
        "                'Outliers': outlier_count,\n",
        "                'Percentage': (outlier_count / len(df)) * 100,\n",
        "                'Lower Bound': lower_bound,\n",
        "                'Upper Bound': upper_bound\n",
        "            })\n",
        "    \n",
        "    if len(outlier_summary) > 0:\n",
        "        outlier_df = pd.DataFrame(outlier_summary)\n",
        "        print(f\"\\nColumns with outliers: {len(outlier_df)}\")\n",
        "        display(outlier_df)\n",
        "    else:\n",
        "        print(\"\\n✓ No outliers detected using IQR method!\")\n",
        "else:\n",
        "    print(\"\\n⚠ No numerical columns to check for outliers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Initial Visualizations\n",
        "\n",
        "Create initial visualizations to understand the data distribution and patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution plots for numerical variables\n",
        "if len(numeric_cols) > 0:\n",
        "    n_cols = min(3, len(numeric_cols))\n",
        "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    axes = axes.flatten() if len(numeric_cols) > 1 else [axes]\n",
        "    \n",
        "    for idx, col in enumerate(numeric_cols):\n",
        "        if idx < len(axes):\n",
        "            df[col].hist(bins=30, ax=axes[idx], edgecolor='black')\n",
        "            axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
        "            axes[idx].set_xlabel(col)\n",
        "            axes[idx].set_ylabel('Frequency')\n",
        "            axes[idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(numeric_cols), len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠ No numerical columns available for distribution plots\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix for numerical variables\n",
        "if len(numeric_cols) > 1:\n",
        "    print(\"=\"*60)\n",
        "    print(\"CORRELATION MATRIX\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Correlation Matrix of Numerical Variables', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nStrong correlations (|r| > 0.7):\")\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            corr_val = corr_matrix.iloc[i, j]\n",
        "            if abs(corr_val) > 0.7:\n",
        "                print(f\"  {corr_matrix.columns[i]} ↔ {corr_matrix.columns[j]}: {corr_val:.3f}\")\n",
        "else:\n",
        "    print(\"⚠ Need at least 2 numerical columns for correlation analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides an initial exploration of the final panel dataset. Key findings and next steps can be documented here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\"*60)\n",
        "print(\"EXPLORATORY ANALYSIS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nDataset loaded successfully:\")\n",
        "print(f\"  - Total rows: {df.shape[0]:,}\")\n",
        "print(f\"  - Total columns: {df.shape[1]}\")\n",
        "print(f\"  - Numerical columns: {len(numeric_cols)}\")\n",
        "print(f\"  - Categorical columns: {len(categorical_cols)}\")\n",
        "print(f\"  - Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"  - Duplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\n✓ Exploratory analysis complete!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  - Perform deeper analysis on key variables\")\n",
        "print(\"  - Create time series visualizations if date data is available\")\n",
        "print(\"  - Analyze relationships between JJM coverage and health outcomes\")\n",
        "print(\"  - Prepare data for econometric modeling\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
